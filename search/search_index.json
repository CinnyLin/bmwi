{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to BMWi Forecasting Tool For full documentation visit mkdocs.org . About the Project This project is a collaboration between German Federal Ministry for Economic Affairs and Energy (BMWi) and Data Science for Social Good Fellowship, UK . The goal of the project is to robustify BMWI\u2019s forecasts by developing a bottom-up forecasting model for predicting economic development in Germany based on regional data. The bottom-up approach focusses The project will draw on fine-grained data on the demographic, economic and sectoral structure of regions with the aim of improving economic forecasts during times of shocks. About the Tool Getting Started There are multiple aspects to be explored with this tool. Head over to this page to get started with the tool. Note: If you wish to get the predictions immediately, you can download them here .","title":"Home"},{"location":"#welcome-to-bmwi-forecasting-tool","text":"For full documentation visit mkdocs.org .","title":"Welcome to BMWi Forecasting Tool"},{"location":"#about-the-project","text":"This project is a collaboration between German Federal Ministry for Economic Affairs and Energy (BMWi) and Data Science for Social Good Fellowship, UK . The goal of the project is to robustify BMWI\u2019s forecasts by developing a bottom-up forecasting model for predicting economic development in Germany based on regional data. The bottom-up approach focusses The project will draw on fine-grained data on the demographic, economic and sectoral structure of regions with the aim of improving economic forecasts during times of shocks.","title":"About the Project"},{"location":"#about-the-tool","text":"","title":"About the Tool"},{"location":"#getting-started","text":"There are multiple aspects to be explored with this tool. Head over to this page to get started with the tool. Note: If you wish to get the predictions immediately, you can download them here .","title":"Getting Started"},{"location":"about/licenses/","text":"","title":"Licences"},{"location":"about/team/","text":"Meet the Team This project has been completed in collaboration with Data Science for Social Good Summer Fellowship. The fellows undertaking this project are: Amit Sasson Cinny Lin Prakhar Rathi Vighnesh N. Ganesh Amit Sasson Cinny Lin Prakhar Rathi Vighnesh Natarajan Ganesh The mentors for this project are: Doschmund Jeyaraj-Kwiatkowski Yurii Tolochko","title":"Team"},{"location":"about/team/#meet-the-team","text":"This project has been completed in collaboration with Data Science for Social Good Summer Fellowship. The fellows undertaking this project are: Amit Sasson Cinny Lin Prakhar Rathi Vighnesh N. Ganesh Amit Sasson Cinny Lin Prakhar Rathi Vighnesh Natarajan Ganesh The mentors for this project are: Doschmund Jeyaraj-Kwiatkowski Yurii Tolochko","title":"Meet the Team"},{"location":"about/tool/","text":"","title":"Tool"},{"location":"advanced/cleanerclass/","text":"CleanerClass Explained This page walks through the assumptions and implementation code for the excel sheet cleaner. Purpose of the CleanerClass The excel sheet cleaner is what is used on the Data Prep page to get the data for our prediction model from excel workbooks. We have written two CleanerClasses: one for labor market data (unemployment rate) and one for GDP . The goal of the CleanerClass is to retrieve a csv-like format of the data, containing only the essential index columns, variable columns, and the numerical values of the data. Worksheets that are metadata descriptions would be discarded, and the watermark-like header rows would also be cropped out in the process. Assumed Formatting for the CleanerClass As explained briefly in the Data Prep page, below is the expected structure of the excel workbook. The assumptions for the input excel workbook are as follows: every kreis is a row although every row need not be a kreis for example, there are also rows that represents the whole bundesland the left most column of the table has the ags5 code value the left most column of the table need not be left most column of sheet \"sheet\" is the data format we are given, and \"table\" is the data format we would end up with thus, once the CleanerClass finds the ags5 column, that column becomes the left most column of the result table, and every column left to the ags5 column would be cropped out in other words, no data should be recorded to the left on the ags5 column also, any essential columns should also be recorded to the right of the ags5 column every column is a time-stamp the input excel workbook is expected to be in wide format the output csv file supports both long and wide formats every cell is a numeric value categorical values are not permitted in the worksheet Code Walkthrough There are only minor differences between the CleanerClass for unemployment rate and the one for GDP. This page would go through a generalized cleanerclass in detail. Class Initiation First, we import the necessary library, pandas , to read in excel workbook in Python. Second, we set a variable called AGS5_LIST , which is a list that stores all the ags5 code in numerical value. # import requirements import pandas as pd #global variable: list of ags5 codes, numeric data type AGS5_LIST = [ 1001, 1002, 1003, 1004, 1051, 1053, 1054, 1055, ...] Then, we start writing the CleanerClass. First, we initiate the attributes for the class. self.sheet : stores the useful worksheets read from the input workbook into a dictionary self.thrownAwayColumns : stores the name and value of the columns that were discarded in each worksheet into a dictionary self.thrownAwayRows : stores the rows that were discarded in each worksheet into a dictionary self.sheet_to_df_map : stores all the worksheets of the workbook into a dictionary, where the key is the name of the worksheet, and the value is the worksheet dataframe class CleanerClass: def __init__(self, file_location, verbose=True): self.verbose = verbose self.sheets = {} # by default we store it in the wide format self.thrownAwayColumns = {} self.thrownAwayRows = {} if verbose: print(\"Reading the file...\") # use sheet_name = None to read all sheets as an ordered dict self.sheet_to_df_map = pd.read_excel(file_location, engine='openpyxl', sheet_name=None) if verbose: print(\"file read complete!\") for sheet_name, sheet_df in self.sheet_to_df_map.items() loops through each worksheet in the workbook using the self.sheet_to_df_map dictionary that we initializes in the last step. sheet = sheet_df.copy() makes a deep copy of each worksheet dataframe, so that any processing made from this point on would not affect the data from the actual worksheet itself. for sheet_name, sheet_df in self.sheet_to_df_map.items(): ## loop through all the sheets sheet = sheet_df.copy() if self.verbose: print(\"\\nprocessing the sheet \", sheet_name, end='') if self.verbose: print('.', end='') 0. Check metadata sheet If the sheet does not have at least 401 rows, it is not a useful sheet. Because we assume the worksheet to follow the format that each kreis is a separate row, and that there are 401 kreise in Germany. This step discards the sheets that contain descriptions instead of numerical-valued data. if sheet.shape[0] < 401: continue # to the next sheet 1. Find the starting column of the table To check if the column is an ags5 column, and not just a column that happen to a variable column that contains the ags5 values. To do this, we randomly selected three ags5 codes from three kreise and checked if all three codes were in the column. If so, it is an ags5 column, and the sheet is one that we can possibly work with. #KREIS_NAMES_TO_CHECK = ['hamburg', 'berlin', 'kiel'] KREIS_CODES_TO_CHECK = ['02000', '11000', '01002'] KREIS_COL_NO = None KREIS_COL_NAME = None for kreis_code in KREIS_CODES_TO_CHECK: for i, col in enumerate(sheet.columns): # find if kreis name is in a substring substring_loc = sheet[col].str.contains(kreis_code, case=False) # returns a number >0 (position where kreis_code starts) found = sum(substring_loc > 0) # if found, just break away, no need to look anymore if found: if KREIS_COL_NO is None: KREIS_COL_NO = i KREIS_COL_NAME = col else: assert KREIS_COL_NO == i break if KREIS_COL_NAME is None: # no column was found with the kreis names # therefore, not a useful sheet continue # to the next sheet if self.verbose: print('.', end='') 2. Find the starting row of the table In step 1 , we found the starting column, ags5 . This step, we find the starting index. To find the starting row, we find the row with dates. Here, we set an arbitrary cutoff at 5. When a row contains at least 5 values that are dates, we consider that row to be the date row with date columns, and consider that row to be the starting row of the table. col_names = sheet.columns DATE_COUNT_CUTOFF = 5 # the number of dates a column must have to qualify as the row which has the table's col names ROW_WITH_DATES = None sheet.reset_index(drop=True, inplace=True) # reset index so that we have the index as a range starting from 0 for index, row in sheet.iterrows(): is_date_arr = [self.is_date(str(row[col_name])) for col_name in col_names] if sum(is_date_arr) >= DATE_COUNT_CUTOFF: # we found the row! ROW_WITH_DATES = index break if ROW_WITH_DATES is None: # no row had dates # therefore, not a table with time series data # therefore not a useful sheet (sheet with needed data) continue # to the next sheet if self.verbose: print('.', end='') 3. Crop out the table 3.1 Crop the columns As explained earlier, step 1 finds the ags5 column, which we consider to be the starting column of the table. This step crops out all the other columns left to the ags5 column. In other words, any important index columns in the worksheet should be recorded on the right to the ags5 column. Note : The CleanerClass currenly does not handle the right boundary of the table because it is automatically handled in the pd.read_excel function. However, if there is any metadata on the right side, it should be cleaned manually before inputting to the cleanerclass. columns_to_drop = [] for col_name in sheet.columns: if col_name == KREIS_COL_NAME: # end the loop break columns_to_drop.append(col_name) sheet.drop(columns_to_drop, axis=1, inplace=True) sheet.set_index(KREIS_COL_NAME, inplace=True) 3.2 Crop the rows As explained earlier, step 2 finds the row that contains the date columns, which we consider to be the starting row of the table. This step crops all the other rows above the date-columns row. sheet.columns = list(sheet.iloc[ROW_WITH_DATES]) sheet = sheet.iloc[ROW_WITH_DATES+1:] sheet.index.name = None if self.verbose: print('.', end='') 4. Throw away rows that are not kreis This step goes through all the rest of the rows below the date-columns row, and discard the ones that are not records on a kreis-level. For implementation, it checks whether the area code in the ags5 column is in the AGS5_LIST we initiated. A special situation to consider are kreise like Hamburg and Berlin, where they are a county (\"kreis\") and a state (\"bundesland\") at the same time. We want to make sure however it is recorded in the worksheet, it would be included to our final table and not accidentally discarded by our cleaner. AGS5_LIST_TEMP = self.AGS5_LIST.copy() #new_index = [] rows_to_drop = [] for index, row in sheet.iterrows(): row_is_needed = False for ags5_loc, ags5 in enumerate(AGS5_LIST_TEMP): if \"{:05d}\".format(ags5) == str(index): AGS5_LIST_TEMP.pop(ags5_loc) # remove that ags5 from AGS5_LIST_TEMP #new_index.append(ags5) # add that index row_is_needed = True # We need this row break # no need to search for other ags5 in AGS5_LIST_TEMP if row_is_needed is False: rows_to_drop.append(index) if len(AGS5_LIST_TEMP) == 2: # berlin and hamburg are not divided down into it's kreis BERLIN_HAMBURG = [2000, 11000] BERLIN_HAMBURG_STR = [\"{:05d}\".format(ags5) for ags5 in BERLIN_HAMBURG] BERLIN_HAMBURG_ags2 = [2, 11] BERLIN_HAMBURG_ags2_STR = [\"{:02d}\".format(ags2) for ags2 in BERLIN_HAMBURG_ags2] ags5s_to_pop = [] for ags5_loc, ags5 in enumerate(AGS5_LIST_TEMP): if ags5 in BERLIN_HAMBURG or BERLIN_HAMBURG_STR: # this bundesland is not then subdivided bacause these 2 bunds. have only one kreis under them # so use the bundesland info as the kreis info ags5s_to_pop.append(ags5_loc) #new_index.append(ags5) for i in range(len(BERLIN_HAMBURG)): if ags5 == BERLIN_HAMBURG[i] or ags5 == BERLIN_HAMBURG_STR[i]: try: rows_to_drop.remove(BERLIN_HAMBURG_ags2[i]) # replace the ags2 index with corresponding ags5 sheet.index = [BERLIN_HAMBURG_STR[i] if ind == BERLIN_HAMBURG_ags2_STR[i] else ind for ind in sheet.index] except: rows_to_drop.remove(BERLIN_HAMBURG_ags2_STR[i]) sheet.index = [BERLIN_HAMBURG_STR[i] if ind == BERLIN_HAMBURG_ags2_STR[i] else ind for ind in sheet.index] break ags5s_to_pop.sort(reverse=True) for ags5_loc in ags5s_to_pop: AGS5_LIST_TEMP.pop(ags5_loc) if len(AGS5_LIST_TEMP) != 0: # some ags5 does not exist! continue # to next sheet sheet.drop(rows_to_drop, axis=0, inplace=True) sheet.index = [int(ind) for ind in sheet.index] self.thrownAwayRows[sheet_name] = rows_to_drop if self.verbose: print('.', end='') 5. Check columns data type This step goes through each columns, and discards the columns where the value is not numerical. The discarded columns are stored in the self.thrownAwayColumns dictionary we initiated at the start. cols_with_non_numeric_vals = [] for col in sheet.columns: try: # find if the col has int or float as dtype: rem = sheet[col] % 1 if sum(rem) == 0: col_type = 'int64' else: col_type = 'float64' sheet[col] = sheet[col].astype(col_type) except: # col cannot be converted to numeric type cols_with_non_numeric_vals.append(col) sheet.drop(cols_with_non_numeric_vals, axis=1, inplace=True) self.thrownAwayColumns[sheet_name] = cols_with_non_numeric_vals if self.verbose: print('.', end='') 6. Drop columns with NA 6.1 Drop all column names as NA Drop the column names that are not a valid date. sheet = sheet[sheet.columns.dropna()] 6.2 Drop columns with NA values Drop the columns with NA values and store the variable names to the self.thrownAwayColumns dictionary we initiated from the start. cols_with_na = sheet.columns[sheet.isnull().any()] sheet.drop(cols_with_na, axis=1, inplace=True) self.thrownAwayColumns[sheet_name].extend(cols_with_na) if self.verbose: print('.', end='') 7. Set the ags5 index to string of length 5 Unify the format of the ags5 codes to be length of 5, data type string. For example, the original area code 1001 , data type int would be converted to 01001 , data type str . sheet.index = [\"{:05d}\".format(ags5) for ags5 in sheet.index] 8. Sort the sheet by ags5 value sheet = sheet.sort_index() 9. Throw away empty sheets If the shape of the sheet is empty, discard this sheet and move on to the next sheet. if 0 in sheet.shape: continue # to next sheet Class Attributes Define functions of the class. In implementation, once a class is called, we can use these pre-defined functions within the class to access the dictionaries and other values we stored in the class initiation steps. is_date This function takes a string and returns whether the string can be interpreted as a date. def is_date(self, string, fuzzy=False): from dateutil.parser import parse if isinstance(string, str): try: parse(string, fuzzy=fuzzy) return True except ValueError: # not a string that can be parsed return False else: # not a string at all return False getAllUsefulSheets_wide The assumed input data format is wide format. Thus, all the useful sheets are stored in wide format by default. def getAllUsefulSheets_wide(self): return self.sheets getAllUsefulSheets_long As explained above, wide format is the assumed input format and default output format. To get long formats of the sheet, we need to do some reshaping of the data. def getAllUsefulSheets_long(self): try: return self.sheets_long except AttributeError: self.sheets_long = {} for df_name, df_og in self.sheets.items(): df = df_og.copy() df = pd.DataFrame([ [ags5, time_stamp, row[time_stamp]] for ags5, row in df.iterrows() for time_stamp in df.columns ]) df.columns = ['ags5', 'time_stamp', 'value'] self.sheets_long[df_name] = df return self.sheets_long getThrownAwayRows The discarded rows are stored in the self.thrownAwayRows dictionary we initiated at the start. def getThrownAwayRows(self): return self.thrownAwayRows getThrownAwayColumns The discarded columns are stored in the self.thrownAwayColumns dictionary we initiated at the start. def getThrownAwayColumns(self): return self.thrownAwayColumns","title":"Cleaner Class"},{"location":"advanced/cleanerclass/#cleanerclass-explained","text":"This page walks through the assumptions and implementation code for the excel sheet cleaner.","title":"CleanerClass Explained"},{"location":"advanced/cleanerclass/#purpose-of-the-cleanerclass","text":"The excel sheet cleaner is what is used on the Data Prep page to get the data for our prediction model from excel workbooks. We have written two CleanerClasses: one for labor market data (unemployment rate) and one for GDP . The goal of the CleanerClass is to retrieve a csv-like format of the data, containing only the essential index columns, variable columns, and the numerical values of the data. Worksheets that are metadata descriptions would be discarded, and the watermark-like header rows would also be cropped out in the process.","title":"Purpose of the CleanerClass"},{"location":"advanced/cleanerclass/#assumed-formatting-for-the-cleanerclass","text":"As explained briefly in the Data Prep page, below is the expected structure of the excel workbook. The assumptions for the input excel workbook are as follows: every kreis is a row although every row need not be a kreis for example, there are also rows that represents the whole bundesland the left most column of the table has the ags5 code value the left most column of the table need not be left most column of sheet \"sheet\" is the data format we are given, and \"table\" is the data format we would end up with thus, once the CleanerClass finds the ags5 column, that column becomes the left most column of the result table, and every column left to the ags5 column would be cropped out in other words, no data should be recorded to the left on the ags5 column also, any essential columns should also be recorded to the right of the ags5 column every column is a time-stamp the input excel workbook is expected to be in wide format the output csv file supports both long and wide formats every cell is a numeric value categorical values are not permitted in the worksheet","title":"Assumed Formatting for the CleanerClass"},{"location":"advanced/cleanerclass/#code-walkthrough","text":"There are only minor differences between the CleanerClass for unemployment rate and the one for GDP. This page would go through a generalized cleanerclass in detail.","title":"Code Walkthrough"},{"location":"advanced/cleanerclass/#class-initiation","text":"First, we import the necessary library, pandas , to read in excel workbook in Python. Second, we set a variable called AGS5_LIST , which is a list that stores all the ags5 code in numerical value. # import requirements import pandas as pd #global variable: list of ags5 codes, numeric data type AGS5_LIST = [ 1001, 1002, 1003, 1004, 1051, 1053, 1054, 1055, ...] Then, we start writing the CleanerClass. First, we initiate the attributes for the class. self.sheet : stores the useful worksheets read from the input workbook into a dictionary self.thrownAwayColumns : stores the name and value of the columns that were discarded in each worksheet into a dictionary self.thrownAwayRows : stores the rows that were discarded in each worksheet into a dictionary self.sheet_to_df_map : stores all the worksheets of the workbook into a dictionary, where the key is the name of the worksheet, and the value is the worksheet dataframe class CleanerClass: def __init__(self, file_location, verbose=True): self.verbose = verbose self.sheets = {} # by default we store it in the wide format self.thrownAwayColumns = {} self.thrownAwayRows = {} if verbose: print(\"Reading the file...\") # use sheet_name = None to read all sheets as an ordered dict self.sheet_to_df_map = pd.read_excel(file_location, engine='openpyxl', sheet_name=None) if verbose: print(\"file read complete!\") for sheet_name, sheet_df in self.sheet_to_df_map.items() loops through each worksheet in the workbook using the self.sheet_to_df_map dictionary that we initializes in the last step. sheet = sheet_df.copy() makes a deep copy of each worksheet dataframe, so that any processing made from this point on would not affect the data from the actual worksheet itself. for sheet_name, sheet_df in self.sheet_to_df_map.items(): ## loop through all the sheets sheet = sheet_df.copy() if self.verbose: print(\"\\nprocessing the sheet \", sheet_name, end='') if self.verbose: print('.', end='')","title":"Class Initiation"},{"location":"advanced/cleanerclass/#0-check-metadata-sheet","text":"If the sheet does not have at least 401 rows, it is not a useful sheet. Because we assume the worksheet to follow the format that each kreis is a separate row, and that there are 401 kreise in Germany. This step discards the sheets that contain descriptions instead of numerical-valued data. if sheet.shape[0] < 401: continue # to the next sheet","title":"0. Check metadata sheet"},{"location":"advanced/cleanerclass/#1-find-the-starting-column-of-the-table","text":"To check if the column is an ags5 column, and not just a column that happen to a variable column that contains the ags5 values. To do this, we randomly selected three ags5 codes from three kreise and checked if all three codes were in the column. If so, it is an ags5 column, and the sheet is one that we can possibly work with. #KREIS_NAMES_TO_CHECK = ['hamburg', 'berlin', 'kiel'] KREIS_CODES_TO_CHECK = ['02000', '11000', '01002'] KREIS_COL_NO = None KREIS_COL_NAME = None for kreis_code in KREIS_CODES_TO_CHECK: for i, col in enumerate(sheet.columns): # find if kreis name is in a substring substring_loc = sheet[col].str.contains(kreis_code, case=False) # returns a number >0 (position where kreis_code starts) found = sum(substring_loc > 0) # if found, just break away, no need to look anymore if found: if KREIS_COL_NO is None: KREIS_COL_NO = i KREIS_COL_NAME = col else: assert KREIS_COL_NO == i break if KREIS_COL_NAME is None: # no column was found with the kreis names # therefore, not a useful sheet continue # to the next sheet if self.verbose: print('.', end='')","title":"1. Find the starting column of the table"},{"location":"advanced/cleanerclass/#2-find-the-starting-row-of-the-table","text":"In step 1 , we found the starting column, ags5 . This step, we find the starting index. To find the starting row, we find the row with dates. Here, we set an arbitrary cutoff at 5. When a row contains at least 5 values that are dates, we consider that row to be the date row with date columns, and consider that row to be the starting row of the table. col_names = sheet.columns DATE_COUNT_CUTOFF = 5 # the number of dates a column must have to qualify as the row which has the table's col names ROW_WITH_DATES = None sheet.reset_index(drop=True, inplace=True) # reset index so that we have the index as a range starting from 0 for index, row in sheet.iterrows(): is_date_arr = [self.is_date(str(row[col_name])) for col_name in col_names] if sum(is_date_arr) >= DATE_COUNT_CUTOFF: # we found the row! ROW_WITH_DATES = index break if ROW_WITH_DATES is None: # no row had dates # therefore, not a table with time series data # therefore not a useful sheet (sheet with needed data) continue # to the next sheet if self.verbose: print('.', end='')","title":"2. Find the starting row of the table"},{"location":"advanced/cleanerclass/#3-crop-out-the-table","text":"","title":"3. Crop out the table"},{"location":"advanced/cleanerclass/#31-crop-the-columns","text":"As explained earlier, step 1 finds the ags5 column, which we consider to be the starting column of the table. This step crops out all the other columns left to the ags5 column. In other words, any important index columns in the worksheet should be recorded on the right to the ags5 column. Note : The CleanerClass currenly does not handle the right boundary of the table because it is automatically handled in the pd.read_excel function. However, if there is any metadata on the right side, it should be cleaned manually before inputting to the cleanerclass. columns_to_drop = [] for col_name in sheet.columns: if col_name == KREIS_COL_NAME: # end the loop break columns_to_drop.append(col_name) sheet.drop(columns_to_drop, axis=1, inplace=True) sheet.set_index(KREIS_COL_NAME, inplace=True)","title":"3.1 Crop the columns"},{"location":"advanced/cleanerclass/#32-crop-the-rows","text":"As explained earlier, step 2 finds the row that contains the date columns, which we consider to be the starting row of the table. This step crops all the other rows above the date-columns row. sheet.columns = list(sheet.iloc[ROW_WITH_DATES]) sheet = sheet.iloc[ROW_WITH_DATES+1:] sheet.index.name = None if self.verbose: print('.', end='')","title":"3.2 Crop the rows"},{"location":"advanced/cleanerclass/#4-throw-away-rows-that-are-not-kreis","text":"This step goes through all the rest of the rows below the date-columns row, and discard the ones that are not records on a kreis-level. For implementation, it checks whether the area code in the ags5 column is in the AGS5_LIST we initiated. A special situation to consider are kreise like Hamburg and Berlin, where they are a county (\"kreis\") and a state (\"bundesland\") at the same time. We want to make sure however it is recorded in the worksheet, it would be included to our final table and not accidentally discarded by our cleaner. AGS5_LIST_TEMP = self.AGS5_LIST.copy() #new_index = [] rows_to_drop = [] for index, row in sheet.iterrows(): row_is_needed = False for ags5_loc, ags5 in enumerate(AGS5_LIST_TEMP): if \"{:05d}\".format(ags5) == str(index): AGS5_LIST_TEMP.pop(ags5_loc) # remove that ags5 from AGS5_LIST_TEMP #new_index.append(ags5) # add that index row_is_needed = True # We need this row break # no need to search for other ags5 in AGS5_LIST_TEMP if row_is_needed is False: rows_to_drop.append(index) if len(AGS5_LIST_TEMP) == 2: # berlin and hamburg are not divided down into it's kreis BERLIN_HAMBURG = [2000, 11000] BERLIN_HAMBURG_STR = [\"{:05d}\".format(ags5) for ags5 in BERLIN_HAMBURG] BERLIN_HAMBURG_ags2 = [2, 11] BERLIN_HAMBURG_ags2_STR = [\"{:02d}\".format(ags2) for ags2 in BERLIN_HAMBURG_ags2] ags5s_to_pop = [] for ags5_loc, ags5 in enumerate(AGS5_LIST_TEMP): if ags5 in BERLIN_HAMBURG or BERLIN_HAMBURG_STR: # this bundesland is not then subdivided bacause these 2 bunds. have only one kreis under them # so use the bundesland info as the kreis info ags5s_to_pop.append(ags5_loc) #new_index.append(ags5) for i in range(len(BERLIN_HAMBURG)): if ags5 == BERLIN_HAMBURG[i] or ags5 == BERLIN_HAMBURG_STR[i]: try: rows_to_drop.remove(BERLIN_HAMBURG_ags2[i]) # replace the ags2 index with corresponding ags5 sheet.index = [BERLIN_HAMBURG_STR[i] if ind == BERLIN_HAMBURG_ags2_STR[i] else ind for ind in sheet.index] except: rows_to_drop.remove(BERLIN_HAMBURG_ags2_STR[i]) sheet.index = [BERLIN_HAMBURG_STR[i] if ind == BERLIN_HAMBURG_ags2_STR[i] else ind for ind in sheet.index] break ags5s_to_pop.sort(reverse=True) for ags5_loc in ags5s_to_pop: AGS5_LIST_TEMP.pop(ags5_loc) if len(AGS5_LIST_TEMP) != 0: # some ags5 does not exist! continue # to next sheet sheet.drop(rows_to_drop, axis=0, inplace=True) sheet.index = [int(ind) for ind in sheet.index] self.thrownAwayRows[sheet_name] = rows_to_drop if self.verbose: print('.', end='')","title":"4. Throw away rows that are not kreis"},{"location":"advanced/cleanerclass/#5-check-columns-data-type","text":"This step goes through each columns, and discards the columns where the value is not numerical. The discarded columns are stored in the self.thrownAwayColumns dictionary we initiated at the start. cols_with_non_numeric_vals = [] for col in sheet.columns: try: # find if the col has int or float as dtype: rem = sheet[col] % 1 if sum(rem) == 0: col_type = 'int64' else: col_type = 'float64' sheet[col] = sheet[col].astype(col_type) except: # col cannot be converted to numeric type cols_with_non_numeric_vals.append(col) sheet.drop(cols_with_non_numeric_vals, axis=1, inplace=True) self.thrownAwayColumns[sheet_name] = cols_with_non_numeric_vals if self.verbose: print('.', end='')","title":"5. Check columns data type"},{"location":"advanced/cleanerclass/#6-drop-columns-with-na","text":"","title":"6. Drop columns with NA"},{"location":"advanced/cleanerclass/#61-drop-all-column-names-as-na","text":"Drop the column names that are not a valid date. sheet = sheet[sheet.columns.dropna()]","title":"6.1 Drop all column names as NA"},{"location":"advanced/cleanerclass/#62-drop-columns-with-na-values","text":"Drop the columns with NA values and store the variable names to the self.thrownAwayColumns dictionary we initiated from the start. cols_with_na = sheet.columns[sheet.isnull().any()] sheet.drop(cols_with_na, axis=1, inplace=True) self.thrownAwayColumns[sheet_name].extend(cols_with_na) if self.verbose: print('.', end='')","title":"6.2 Drop columns with NA values"},{"location":"advanced/cleanerclass/#7-set-the-ags5-index-to-string-of-length-5","text":"Unify the format of the ags5 codes to be length of 5, data type string. For example, the original area code 1001 , data type int would be converted to 01001 , data type str . sheet.index = [\"{:05d}\".format(ags5) for ags5 in sheet.index]","title":"7. Set the ags5 index to string of length 5"},{"location":"advanced/cleanerclass/#8-sort-the-sheet-by-ags5-value","text":"sheet = sheet.sort_index()","title":"8. Sort the sheet by ags5 value"},{"location":"advanced/cleanerclass/#9-throw-away-empty-sheets","text":"If the shape of the sheet is empty, discard this sheet and move on to the next sheet. if 0 in sheet.shape: continue # to next sheet","title":"9. Throw away empty sheets"},{"location":"advanced/cleanerclass/#class-attributes","text":"Define functions of the class. In implementation, once a class is called, we can use these pre-defined functions within the class to access the dictionaries and other values we stored in the class initiation steps.","title":"Class Attributes"},{"location":"advanced/cleanerclass/#is_date","text":"This function takes a string and returns whether the string can be interpreted as a date. def is_date(self, string, fuzzy=False): from dateutil.parser import parse if isinstance(string, str): try: parse(string, fuzzy=fuzzy) return True except ValueError: # not a string that can be parsed return False else: # not a string at all return False","title":"is_date"},{"location":"advanced/cleanerclass/#getallusefulsheets_wide","text":"The assumed input data format is wide format. Thus, all the useful sheets are stored in wide format by default. def getAllUsefulSheets_wide(self): return self.sheets","title":"getAllUsefulSheets_wide"},{"location":"advanced/cleanerclass/#getallusefulsheets_long","text":"As explained above, wide format is the assumed input format and default output format. To get long formats of the sheet, we need to do some reshaping of the data. def getAllUsefulSheets_long(self): try: return self.sheets_long except AttributeError: self.sheets_long = {} for df_name, df_og in self.sheets.items(): df = df_og.copy() df = pd.DataFrame([ [ags5, time_stamp, row[time_stamp]] for ags5, row in df.iterrows() for time_stamp in df.columns ]) df.columns = ['ags5', 'time_stamp', 'value'] self.sheets_long[df_name] = df return self.sheets_long","title":"getAllUsefulSheets_long"},{"location":"advanced/cleanerclass/#getthrownawayrows","text":"The discarded rows are stored in the self.thrownAwayRows dictionary we initiated at the start. def getThrownAwayRows(self): return self.thrownAwayRows","title":"getThrownAwayRows"},{"location":"advanced/cleanerclass/#getthrownawaycolumns","text":"The discarded columns are stored in the self.thrownAwayColumns dictionary we initiated at the start. def getThrownAwayColumns(self): return self.thrownAwayColumns","title":"getThrownAwayColumns"},{"location":"advanced/installation/","text":"Technical Documentation This section is for people who might want to edit the code base of the application. The repository with all the code can be found here . The application is built using the following tools and frameworks: Python Streamlit Pandas Statsmodels Keras To make edits to the tool, one must know how to work with the aforementioned frameworks. More details about each of them can be found on the links provided above. About Streamlit This application is built using Streamlit , a Python framework to build interactive web applications. It has a rich API for various interactive features which can be learnt more about here . It's a relatively new framework and its recommended to explore building basic apps with it before diving deep into the repository. To set up the repository on your local machine follow the installation steps below: Installation To set up the application on your local machine, follow these steps. Clone the repository on your local machine git clone link Create a virtual environment For MacOS/Linux python3 -m venv env For Windows py -m venv env The second argument is the location to create the virtual environment. Generally, you can just create this in your project and call it env . Read more about virtual env here . Note: You should exclude your virtual environment directory from your version control system using .gitignore or similar. Activate the virtual environment For MacOS/Linux source env/bin/activate For Windows .\\env\\Scripts\\activate Install requirements pip install -r requirements.txt Run the streamlit app streamlit run app.py Code Walkthrough Page by page walkthrough of the functions being used in the application.","title":"Installation and Setup"},{"location":"advanced/installation/#technical-documentation","text":"This section is for people who might want to edit the code base of the application. The repository with all the code can be found here . The application is built using the following tools and frameworks: Python Streamlit Pandas Statsmodels Keras To make edits to the tool, one must know how to work with the aforementioned frameworks. More details about each of them can be found on the links provided above.","title":"Technical Documentation"},{"location":"advanced/installation/#about-streamlit","text":"This application is built using Streamlit , a Python framework to build interactive web applications. It has a rich API for various interactive features which can be learnt more about here . It's a relatively new framework and its recommended to explore building basic apps with it before diving deep into the repository. To set up the repository on your local machine follow the installation steps below:","title":"About Streamlit"},{"location":"advanced/installation/#installation","text":"To set up the application on your local machine, follow these steps. Clone the repository on your local machine git clone link Create a virtual environment For MacOS/Linux python3 -m venv env For Windows py -m venv env The second argument is the location to create the virtual environment. Generally, you can just create this in your project and call it env . Read more about virtual env here . Note: You should exclude your virtual environment directory from your version control system using .gitignore or similar. Activate the virtual environment For MacOS/Linux source env/bin/activate For Windows .\\env\\Scripts\\activate Install requirements pip install -r requirements.txt Run the streamlit app streamlit run app.py","title":"Installation"},{"location":"advanced/installation/#code-walkthrough","text":"Page by page walkthrough of the functions being used in the application.","title":"Code Walkthrough"},{"location":"advanced/tech_flow/","text":"Technical Workflow mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Pro Tip : Clicking on the purple nodes leads to the source code. graph TD subgraph Dataset Prep page A[time-series excel workbook with CleanerClass] end subgraph Model page B[Unemployment Rate Prediction on Kreis-level] C[Visualization: line plot, map of Germany] D[Output file: single excel worksheet] B --> C --> D end subgraph Error Analysis page E[compare structure, crisis-time data] end subgraph Home page F[rankings & groups] end A --> B D --> E D --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click C \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click D \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\"","title":"Technical Workflow"},{"location":"advanced/tech_flow/#technical-workflow","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Pro Tip : Clicking on the purple nodes leads to the source code. graph TD subgraph Dataset Prep page A[time-series excel workbook with CleanerClass] end subgraph Model page B[Unemployment Rate Prediction on Kreis-level] C[Visualization: line plot, map of Germany] D[Output file: single excel worksheet] B --> C --> D end subgraph Error Analysis page E[compare structure, crisis-time data] end subgraph Home page F[rankings & groups] end A --> B D --> E D --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click C \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click D \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\"","title":"Technical Workflow"},{"location":"journey/causal_inf/","text":"","title":"Causal Inference"},{"location":"journey/clusters/","text":"Cluster Analysis Need for clusters Type of Clusters Cluster graphs Cluster groups","title":"Clusters"},{"location":"journey/clusters/#cluster-analysis","text":"","title":"Cluster Analysis"},{"location":"journey/clusters/#need-for-clusters","text":"","title":"Need for clusters"},{"location":"journey/clusters/#type-of-clusters","text":"","title":"Type of Clusters"},{"location":"journey/clusters/#cluster-graphs","text":"","title":"Cluster graphs"},{"location":"journey/clusters/#cluster-groups","text":"","title":"Cluster groups"},{"location":"journey/models/","text":"Model Descriptions This section talks about the various models that we have built and tested. It documents the various models which worked well and also the models which did not work well. Vector Autoregression Vector Autoregression (VAR) is a forecasting algorithm that can be used when two or more time series influence each other. That is, the relationship between the time series involved is bi-directional. Model Introduction Vector Autoregression (VAR) is a multivariate forecasting algorithm that is used when two or more time series influence each other. It is considered as an Autoregressive model because, each variable (Time Series) is modeled as a function of the past values, that is the predictors are nothing but the lags (time delayed value) of the series.The primary difference between VAR and other Autoregressive (AR) modesl is that these models are uni-directional, where, the predictors influence the Y and not vice-versa. Whereas, VAR is bi-directional. That is, the variables influence each other. Typical AR Model: Future value only depends on the past values of that series. VAR Model: Future values of one series depends on the past value of the same series and the parallel series Training Method The training is done using the clusters that were developed based on the stationary data. The cluster development method has been elucidated within the cluster section . The intuition behind VAR based cluster methods is that the Kreis in each cluster move together in terms of the unemployment rate and affect each other. The regional clusters are made using Bundesland and does not perform as well as the custom clusters which were created using statistical data. Since VAR learns from parallel series, the kreis in each cluster affect each other's unemployment rate within the same cluster. Should I explain walk-forward? Prophet References [1] https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/","title":"Models"},{"location":"journey/models/#model-descriptions","text":"This section talks about the various models that we have built and tested. It documents the various models which worked well and also the models which did not work well.","title":"Model Descriptions"},{"location":"journey/models/#vector-autoregression","text":"Vector Autoregression (VAR) is a forecasting algorithm that can be used when two or more time series influence each other. That is, the relationship between the time series involved is bi-directional.","title":"Vector Autoregression"},{"location":"journey/models/#model-introduction","text":"Vector Autoregression (VAR) is a multivariate forecasting algorithm that is used when two or more time series influence each other. It is considered as an Autoregressive model because, each variable (Time Series) is modeled as a function of the past values, that is the predictors are nothing but the lags (time delayed value) of the series.The primary difference between VAR and other Autoregressive (AR) modesl is that these models are uni-directional, where, the predictors influence the Y and not vice-versa. Whereas, VAR is bi-directional. That is, the variables influence each other. Typical AR Model: Future value only depends on the past values of that series. VAR Model: Future values of one series depends on the past value of the same series and the parallel series","title":"Model Introduction"},{"location":"journey/models/#training-method","text":"The training is done using the clusters that were developed based on the stationary data. The cluster development method has been elucidated within the cluster section . The intuition behind VAR based cluster methods is that the Kreis in each cluster move together in terms of the unemployment rate and affect each other. The regional clusters are made using Bundesland and does not perform as well as the custom clusters which were created using statistical data. Since VAR learns from parallel series, the kreis in each cluster affect each other's unemployment rate within the same cluster. Should I explain walk-forward?","title":"Training Method"},{"location":"journey/models/#prophet","text":"","title":"Prophet"},{"location":"journey/models/#references","text":"[1] https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/","title":"References"},{"location":"start/quick/","text":"Quick Access Welcome to the BMWi tool documentation. This is a tool developed to forecast the Kreis level unemployment rate for the next quarter as well as perform various analyses like the prediction error analysis or Kreis vulnerability detection using structural data. There are different things you can explore depending on the use case. The common ones are mentioned below: If you want the latest unemployment rate predictions, then you can visit this page . If you want understand the step-by-step procedure to get fresh predictions, head to the Step-By-Step Guide . To set up the tool on your local machine, follow the installation guide . If you want to check out the techincal documentation and understand the prediction functions, visit the techical documentation page. If you want to get more details about the data cleaning, analysis and the machine learning models that we built, head over to our journey section. To know more about the team which developed this application, check out the Team Page . Add a download link here","title":"Quick Access"},{"location":"start/quick/#quick-access","text":"Welcome to the BMWi tool documentation. This is a tool developed to forecast the Kreis level unemployment rate for the next quarter as well as perform various analyses like the prediction error analysis or Kreis vulnerability detection using structural data. There are different things you can explore depending on the use case. The common ones are mentioned below: If you want the latest unemployment rate predictions, then you can visit this page . If you want understand the step-by-step procedure to get fresh predictions, head to the Step-By-Step Guide . To set up the tool on your local machine, follow the installation guide . If you want to check out the techincal documentation and understand the prediction functions, visit the techical documentation page. If you want to get more details about the data cleaning, analysis and the machine learning models that we built, head over to our journey section. To know more about the team which developed this application, check out the Team Page . Add a download link here","title":"Quick Access"},{"location":"start/start/","text":"Start Here mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); We have identified four potential types of users that would benefit from this documentation and tool. If you have a suggestion of another type of user we did not think of, please let us know. \ud83e\udd2f First-time Users | \ud83d\ude0a Experienced Users | \ud83d\ude05 Quick Access | \ud83e\udd14 Technical Users Pro Tip : Clicking on the blue boxes in the flowchart brings you to the documentation page for that specific step. First-time Users This is a simplified version of the user workflow. graph LR subgraph Dataset Prep page A[data input] end subgraph Model page B[prediction] end subgraph Error Analysis page E[compare results] end subgraph Home page F[data story-telling] end F -.-> A A --> B B --> E B --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\" When you open the tool, the first page you see is the Home page. Your prediction journey starts on the Data Prep page. There, you upload the data, and do necessary preprocessing that would then feed into the model. Once you \"confirm\" the preprocessed data on the Data Prep page, you can go to the Model page. The preprocessed data from the page before is automatically loaded. The predictions may take a while to run. The prediction results are cached, which means it should run faster the second time you try to predict the same data. On the same model page, there are some line plots and map visualizations to quickly understand the prediction results, e.g. which kreis has the highest unemployment rate, whether the trend for that kreis is going up or down. The Error Analysis page would be automatically loaded with the prediction results. This page helps you look closer into which kreise were harder to predict, and how that compares with their basic infrastructures, such as number of hospitals, number of schools etc. The Home page is also automatically loaded with the prediction results. Apart from the map and line plot that you immediately saw on the model page, the home page contains kreis-level and grouped rankings of unemployment rates and their percentage changes. The home page is set to be the first page of the tool for quick access. graph LR A1(First-time Users) A1-->A2[Step-By-Step Guide] A1-->A3[Tool] click A2 \"http://127.0.0.1:8000/steps/home/\" click A3 \"https://bmwi-viz.herokuapp.com/\" style A2 fill:#CAEEFE,stroke:#2596be,color:#063970 style A3 fill:#CAEEFE,stroke:#2596be,color:#063970 Now that you understand what you can expect, visit to the Step-By-Step Guide section of the documentation. We suggest that you open up the tool on a side-by-side window, so that you can follow and implement along with the guided instruction. Experienced Users If you are an experienced user, you can dive right in the tool! graph LR B1(Experienced Users) B1-->B2[Tool] B1-->B3[Error Handling] click B2 \"https://bmwi-viz.herokuapp.com/\" click B3 \"http://127.0.0.1:8000/steps/home/\" style B2 fill:#CAEEFE,stroke:#2596be,color:#063970 style B3 fill:#CAEEFE,stroke:#2596be,color:#063970 If you are encountering problems, it is likely that the problem and how to solve it is already noted in our documentation. You can visit the specific page in the Step-By-Step Guide section . If you are encountering a problem that is not recorded in our documentation, please let us know. Quick Access Check out the Quick Access pages we built just for you! graph LR C1(Quick Access) C1 --> C2[Prediction Results] C1 --> C3[Quick Access documentation] C1 --> C4[Quick Access tool page] click C2 \"http://127.0.0.1:8000/start/start/\" click C3 \"http://127.0.0.1:8000/start/quick/\" click C4 \"http://localhost:8501/\" style C2 fill:#CAEEFE,stroke:#2596be,color:#063970 style C3 fill:#CAEEFE,stroke:#2596be,color:#063970 style C4 fill:#CAEEFE,stroke:#2596be,color:#063970 If you just want to get the prediction results for this quarter, here is the link to download the excel file. If you want to quickly get a grasp of the project, the tool, and the documentation, this page is for you. If you want to get a light interpreattion for the latest prediction results, the home page of the tool provides rankings, and the model page provides line plots and a map of Germany. Technical Users The Advanced Features section contains detailed code walkthroughs. graph LR D1(Technical Users) D1 --> D3[Technical Workflow page] D1 --> D4[Installation and Setup page] click D3 \"http://127.0.0.1:8000/advanced/tech_flow/\" click D4 \"http://127.0.0.1:8000/advanced/installation/\" style D3 fill:#CAEEFE,stroke:#2596be,color:#063970 style D4 fill:#CAEEFE,stroke:#2596be,color:#063970 The Technical Workflow page explains the data model pipeline. Check out the Installation and Setup page to get started!","title":"Start Here"},{"location":"start/start/#start-here","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); We have identified four potential types of users that would benefit from this documentation and tool. If you have a suggestion of another type of user we did not think of, please let us know. \ud83e\udd2f First-time Users | \ud83d\ude0a Experienced Users | \ud83d\ude05 Quick Access | \ud83e\udd14 Technical Users Pro Tip : Clicking on the blue boxes in the flowchart brings you to the documentation page for that specific step.","title":"Start Here"},{"location":"start/start/#first-time-users","text":"This is a simplified version of the user workflow. graph LR subgraph Dataset Prep page A[data input] end subgraph Model page B[prediction] end subgraph Error Analysis page E[compare results] end subgraph Home page F[data story-telling] end F -.-> A A --> B B --> E B --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\" When you open the tool, the first page you see is the Home page. Your prediction journey starts on the Data Prep page. There, you upload the data, and do necessary preprocessing that would then feed into the model. Once you \"confirm\" the preprocessed data on the Data Prep page, you can go to the Model page. The preprocessed data from the page before is automatically loaded. The predictions may take a while to run. The prediction results are cached, which means it should run faster the second time you try to predict the same data. On the same model page, there are some line plots and map visualizations to quickly understand the prediction results, e.g. which kreis has the highest unemployment rate, whether the trend for that kreis is going up or down. The Error Analysis page would be automatically loaded with the prediction results. This page helps you look closer into which kreise were harder to predict, and how that compares with their basic infrastructures, such as number of hospitals, number of schools etc. The Home page is also automatically loaded with the prediction results. Apart from the map and line plot that you immediately saw on the model page, the home page contains kreis-level and grouped rankings of unemployment rates and their percentage changes. The home page is set to be the first page of the tool for quick access. graph LR A1(First-time Users) A1-->A2[Step-By-Step Guide] A1-->A3[Tool] click A2 \"http://127.0.0.1:8000/steps/home/\" click A3 \"https://bmwi-viz.herokuapp.com/\" style A2 fill:#CAEEFE,stroke:#2596be,color:#063970 style A3 fill:#CAEEFE,stroke:#2596be,color:#063970 Now that you understand what you can expect, visit to the Step-By-Step Guide section of the documentation. We suggest that you open up the tool on a side-by-side window, so that you can follow and implement along with the guided instruction.","title":"First-time Users"},{"location":"start/start/#experienced-users","text":"If you are an experienced user, you can dive right in the tool! graph LR B1(Experienced Users) B1-->B2[Tool] B1-->B3[Error Handling] click B2 \"https://bmwi-viz.herokuapp.com/\" click B3 \"http://127.0.0.1:8000/steps/home/\" style B2 fill:#CAEEFE,stroke:#2596be,color:#063970 style B3 fill:#CAEEFE,stroke:#2596be,color:#063970 If you are encountering problems, it is likely that the problem and how to solve it is already noted in our documentation. You can visit the specific page in the Step-By-Step Guide section . If you are encountering a problem that is not recorded in our documentation, please let us know.","title":"Experienced Users"},{"location":"start/start/#quick-access","text":"Check out the Quick Access pages we built just for you! graph LR C1(Quick Access) C1 --> C2[Prediction Results] C1 --> C3[Quick Access documentation] C1 --> C4[Quick Access tool page] click C2 \"http://127.0.0.1:8000/start/start/\" click C3 \"http://127.0.0.1:8000/start/quick/\" click C4 \"http://localhost:8501/\" style C2 fill:#CAEEFE,stroke:#2596be,color:#063970 style C3 fill:#CAEEFE,stroke:#2596be,color:#063970 style C4 fill:#CAEEFE,stroke:#2596be,color:#063970 If you just want to get the prediction results for this quarter, here is the link to download the excel file. If you want to quickly get a grasp of the project, the tool, and the documentation, this page is for you. If you want to get a light interpreattion for the latest prediction results, the home page of the tool provides rankings, and the model page provides line plots and a map of Germany.","title":"Quick Access"},{"location":"start/start/#technical-users","text":"The Advanced Features section contains detailed code walkthroughs. graph LR D1(Technical Users) D1 --> D3[Technical Workflow page] D1 --> D4[Installation and Setup page] click D3 \"http://127.0.0.1:8000/advanced/tech_flow/\" click D4 \"http://127.0.0.1:8000/advanced/installation/\" style D3 fill:#CAEEFE,stroke:#2596be,color:#063970 style D4 fill:#CAEEFE,stroke:#2596be,color:#063970 The Technical Workflow page explains the data model pipeline. Check out the Installation and Setup page to get started!","title":"Technical Users"},{"location":"start/user/","text":"User Worlflow mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Pro Tip : Clicking on the purple nodes leads to the tool page. graph TD subgraph Dataset Prep page A[time-series excel workbook OR structural data csv files] end subgraph Model page B[Unemployment Rate Prediction on Kreis-level] C[Visualization: line plot, map of Germany] D[Output file: single excel worksheet] B --> C --> D end subgraph Error Analysis page E[Compare: structural, crisis-time data] end subgraph Home page F[Rankings & Groupings] end A --> B D --> E D --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click C \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click D \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\"","title":"User Workflow"},{"location":"start/user/#user-worlflow","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Pro Tip : Clicking on the purple nodes leads to the tool page. graph TD subgraph Dataset Prep page A[time-series excel workbook OR structural data csv files] end subgraph Model page B[Unemployment Rate Prediction on Kreis-level] C[Visualization: line plot, map of Germany] D[Output file: single excel worksheet] B --> C --> D end subgraph Error Analysis page E[Compare: structural, crisis-time data] end subgraph Home page F[Rankings & Groupings] end A --> B D --> E D --> F click A \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/data_prep.py\" click B \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click C \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click D \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/model_v1.py\" click E \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/error_analysis.py\" click F \"https://github.com/prakharrathi25/the-tool-bmwi/blob/main/pages/home_page.py\"","title":"User Worlflow"},{"location":"steps/data_prep/","text":"Data Prep page Upload the dataset and make any necessary changes to fit the prediction model. Reshape and merge datasets This section process data differently for excel or csv files, and for time-series or structural data type. It assumes that: time-series data are in wide -format excel workbooks, where each worksheet contains data for one variable structural data are in long -format csv files, where each row is a record of one kreis, and each column is one variable both assumes to have one column containing the kreis-level area code ags5 (\"amtlicher gemeindeschl\u00fcssel\") Time-series data excel workbook Error Handling When you open up the page, you will likely be facing this error. No worries, that just means you haven't uploaded a data yet! Upload Excel Workbook When you confirm that you are using an excel workbook containing time-series data, you are prompted to upload the data. In this sample screenshot, you can see that the file \"7444_318010_BMWI_Enkelmann_Eckdaten_Zeitreihe_Kreise.xlsx\" is being uploaded. Select data to clean Because each data is formatted differently in the excel workbook, we created different cleaners for different data. Select the appropriate cleaner based on the data input. Currently, we support: Unemployment rate (labor market data, containing unemployment rate information) This is the assumed workbook for labor market data: The index column \"Region\" shows that each row is a record of one kreis. The rest of the columns should represent each year and month. Each worksheet represents a different variable. For example, the current worksheet selected is \"Alo_Quote\", which stands for unemployment rate (\"Arbeitslosenquote\"). GDP (GDP data, containing kreis-level GDP breakdown) This is the assumed workbook for GDP data: The index column is \"Regional-schl\u00fcssel\". We are looking for the ones with 5 numbers, which means they are on the kreis-level (corresponding to the \"NUTS 3\" column). The rest of the columns should represent each year. Each worksheet represents a different variable. For example, the current worksheet selected is \"1.1\", which according to the trailing title means gross domestic product (\"bruttoinlandsprodukt\"). Select variable Choose at least one variable. Choosing no variables at all would result in the IndexError above. Select data format Long format This is an example of a single-variable \"long\" format data. The dataframe contains three columns: the kreis code ( ags5 ), the time ( date ), and the variable (e.g. Alo Quote ). The number of rows of this file is 401 kreis * #dates for each kreis. Wide format This is an example of a single-variable \"wide\" format data. All the columns in the dataframe are dates. The index of the dataframe is the ags5 code. The number of rows of this file should be 401. Structural data csv files This is a sample of a csv file the tool expects. It should at least contain ags5 , but could also include metadata such as ags2 , bundesland , kreis etc. Each column afterwards is a varaible. A simple merge using the ags5 column, we have a merged file of 401 rows, and the combination of all the selected columns. In this example, we merged two files, raumordnung and point_of_interest . Again, a preview of the merged file would be shownn, and once confirmed, it would be the default loaded dataset. Final dataset cleaning Data cleaning such as cropping to a certain dataframe, checking for NaN data etc. Loading dataset Here, you load the data you would like to clean. By default, it loads the data that you have confirmed processing and merging in the last section. Cropping timeframe Pro Tip : The purpose of cropping the appropriate timeframe helps differentiate between a normal-time and crisis-time model. For example, we know that there was the economic crisis around 2008 and the COVID pandemic around 2020. Thus, when doing a normal-time unemployment rate prediction, you could crop out those times so that the model is only learning the pattern from normal-time data. This is where you can select the timeframe you would like to feed into the model. In this example, a timeframe between 2007-05 and 2007-07 is selected and shown in the preview. Again, once you confirm the dataset, it would be automatically loaded in the model on the next page.","title":"Data Prep page"},{"location":"steps/data_prep/#data-prep-page","text":"Upload the dataset and make any necessary changes to fit the prediction model.","title":"Data Prep page"},{"location":"steps/data_prep/#reshape-and-merge-datasets","text":"This section process data differently for excel or csv files, and for time-series or structural data type. It assumes that: time-series data are in wide -format excel workbooks, where each worksheet contains data for one variable structural data are in long -format csv files, where each row is a record of one kreis, and each column is one variable both assumes to have one column containing the kreis-level area code ags5 (\"amtlicher gemeindeschl\u00fcssel\")","title":"Reshape and merge datasets"},{"location":"steps/data_prep/#time-series-data-excel-workbook","text":"","title":"Time-series data excel workbook"},{"location":"steps/data_prep/#error-handling","text":"When you open up the page, you will likely be facing this error. No worries, that just means you haven't uploaded a data yet!","title":"Error Handling"},{"location":"steps/data_prep/#upload-excel-workbook","text":"When you confirm that you are using an excel workbook containing time-series data, you are prompted to upload the data. In this sample screenshot, you can see that the file \"7444_318010_BMWI_Enkelmann_Eckdaten_Zeitreihe_Kreise.xlsx\" is being uploaded.","title":"Upload Excel Workbook"},{"location":"steps/data_prep/#select-data-to-clean","text":"Because each data is formatted differently in the excel workbook, we created different cleaners for different data. Select the appropriate cleaner based on the data input. Currently, we support: Unemployment rate (labor market data, containing unemployment rate information) This is the assumed workbook for labor market data: The index column \"Region\" shows that each row is a record of one kreis. The rest of the columns should represent each year and month. Each worksheet represents a different variable. For example, the current worksheet selected is \"Alo_Quote\", which stands for unemployment rate (\"Arbeitslosenquote\"). GDP (GDP data, containing kreis-level GDP breakdown) This is the assumed workbook for GDP data: The index column is \"Regional-schl\u00fcssel\". We are looking for the ones with 5 numbers, which means they are on the kreis-level (corresponding to the \"NUTS 3\" column). The rest of the columns should represent each year. Each worksheet represents a different variable. For example, the current worksheet selected is \"1.1\", which according to the trailing title means gross domestic product (\"bruttoinlandsprodukt\").","title":"Select data to clean"},{"location":"steps/data_prep/#select-variable","text":"Choose at least one variable. Choosing no variables at all would result in the IndexError above.","title":"Select variable"},{"location":"steps/data_prep/#select-data-format","text":"","title":"Select data format"},{"location":"steps/data_prep/#long-format","text":"This is an example of a single-variable \"long\" format data. The dataframe contains three columns: the kreis code ( ags5 ), the time ( date ), and the variable (e.g. Alo Quote ). The number of rows of this file is 401 kreis * #dates for each kreis.","title":"Long format"},{"location":"steps/data_prep/#wide-format","text":"This is an example of a single-variable \"wide\" format data. All the columns in the dataframe are dates. The index of the dataframe is the ags5 code. The number of rows of this file should be 401.","title":"Wide format"},{"location":"steps/data_prep/#structural-data-csv-files","text":"This is a sample of a csv file the tool expects. It should at least contain ags5 , but could also include metadata such as ags2 , bundesland , kreis etc. Each column afterwards is a varaible. A simple merge using the ags5 column, we have a merged file of 401 rows, and the combination of all the selected columns. In this example, we merged two files, raumordnung and point_of_interest . Again, a preview of the merged file would be shownn, and once confirmed, it would be the default loaded dataset.","title":"Structural data csv files"},{"location":"steps/data_prep/#final-dataset-cleaning","text":"Data cleaning such as cropping to a certain dataframe, checking for NaN data etc.","title":"Final dataset cleaning"},{"location":"steps/data_prep/#loading-dataset","text":"Here, you load the data you would like to clean. By default, it loads the data that you have confirmed processing and merging in the last section.","title":"Loading dataset"},{"location":"steps/data_prep/#cropping-timeframe","text":"Pro Tip : The purpose of cropping the appropriate timeframe helps differentiate between a normal-time and crisis-time model. For example, we know that there was the economic crisis around 2008 and the COVID pandemic around 2020. Thus, when doing a normal-time unemployment rate prediction, you could crop out those times so that the model is only learning the pattern from normal-time data. This is where you can select the timeframe you would like to feed into the model. In this example, a timeframe between 2007-05 and 2007-07 is selected and shown in the preview. Again, once you confirm the dataset, it would be automatically loaded in the model on the next page.","title":"Cropping timeframe"},{"location":"steps/error/","text":"Error Analysis Page Identify the kreis that are hard to get predictions for. The error analysis page uses the errors generated from the latest predictions and compares them with the structural data so that you can idenitfy which Kreis are harder to predict unemployment for and see what is different in those Kreis as opposed to the other Kreis. Note: The errors calculated for the previous quarter based on the recently uploaded data and, hence, this is a retrospective analysis. One can identify which kreis were harder to predict for in the previous quarter. Launching the page To open the error analysis page, select the 'Error Analysis` Section from the dropdown on the left as can be viewed in the image below. There are a variety of different plots that can be explored and have been explained in detail below. Map Visualisation This is the first step of visualisation that one can view. Select the checkbox Visualize error on a map? . This will toggle open the following section. Here, you will see the average option selected by default. This will make a map with the average value of errors. This average is calculated over all the dates of the predictions. On clicking on the dropdown and selecting a specific date, the errors for that date will be plotted as seen below. Error Plots by Bundesland This section allows for a kreis-level or bundesland-level analysis. The left dropdown allows selection by Kreis or by Bundesland and the right dropdown allows selection of all or a particular region. Selecting all displays every kreis or bundesland in the same graph. Selecting an individual entry would plot the errors for a specific kreis or bundesland. The individual entries can be selected from the right dropdown option. Kreis Level Overview The goal of the application is to break down the predictions as well as the errors at the Kreis level. The following section performs error data analysis. It helps in understanding which Kreis are the hardest to estimate for unemployment rate. This dataframe currently shows 5 kreis based on their unemployment rate forecasting errors for previous quarter. There are two configuration options here: - Highest or Lowest : This lets you select the Kreis with the highest or lowest unemployment rate prediction errors in the previous quarter. - Value Slider : This lets you select the number of Kreis to be displayed currently. From UI perspective, the maximum limit is 20 To view all the Kreis, download the complete error table by clicking on the option Download the full error table . Structural Data Analysis The next step is to analyze the errors with regards to the structural data and see hot the errors vary with a particular structural variable. Select a structural variable to compare against the errors. ( Eg: Errors are compared against eligible_area in this image ). The errors are on the x-axis so the fatter the curve is the more prone that category is to errors. In the following example, the areas where eligible_area is 0 are less prone to prediction errors than areas with code 1. Future: Later, we can add a check to see if the variable is a categorical or numerical variable. Most important Structural Variables Individual exploration might be interesting but can also be very time consuming. There is an option to get an overview of which features are most linearly related to the prediction errors. The features are then listed in the order of importance. There is a provision to choose how many variables to show. Explain how they have been calculated.","title":"Error Analysis page"},{"location":"steps/error/#error-analysis-page","text":"Identify the kreis that are hard to get predictions for. The error analysis page uses the errors generated from the latest predictions and compares them with the structural data so that you can idenitfy which Kreis are harder to predict unemployment for and see what is different in those Kreis as opposed to the other Kreis. Note: The errors calculated for the previous quarter based on the recently uploaded data and, hence, this is a retrospective analysis. One can identify which kreis were harder to predict for in the previous quarter.","title":"Error Analysis Page"},{"location":"steps/error/#launching-the-page","text":"To open the error analysis page, select the 'Error Analysis` Section from the dropdown on the left as can be viewed in the image below. There are a variety of different plots that can be explored and have been explained in detail below.","title":"Launching the page"},{"location":"steps/error/#map-visualisation","text":"This is the first step of visualisation that one can view. Select the checkbox Visualize error on a map? . This will toggle open the following section. Here, you will see the average option selected by default. This will make a map with the average value of errors. This average is calculated over all the dates of the predictions. On clicking on the dropdown and selecting a specific date, the errors for that date will be plotted as seen below.","title":"Map Visualisation"},{"location":"steps/error/#error-plots-by-bundesland","text":"This section allows for a kreis-level or bundesland-level analysis. The left dropdown allows selection by Kreis or by Bundesland and the right dropdown allows selection of all or a particular region. Selecting all displays every kreis or bundesland in the same graph. Selecting an individual entry would plot the errors for a specific kreis or bundesland. The individual entries can be selected from the right dropdown option.","title":"Error Plots by Bundesland"},{"location":"steps/error/#kreis-level-overview","text":"The goal of the application is to break down the predictions as well as the errors at the Kreis level. The following section performs error data analysis. It helps in understanding which Kreis are the hardest to estimate for unemployment rate. This dataframe currently shows 5 kreis based on their unemployment rate forecasting errors for previous quarter. There are two configuration options here: - Highest or Lowest : This lets you select the Kreis with the highest or lowest unemployment rate prediction errors in the previous quarter. - Value Slider : This lets you select the number of Kreis to be displayed currently. From UI perspective, the maximum limit is 20 To view all the Kreis, download the complete error table by clicking on the option Download the full error table .","title":"Kreis Level Overview"},{"location":"steps/error/#structural-data-analysis","text":"The next step is to analyze the errors with regards to the structural data and see hot the errors vary with a particular structural variable. Select a structural variable to compare against the errors. ( Eg: Errors are compared against eligible_area in this image ). The errors are on the x-axis so the fatter the curve is the more prone that category is to errors. In the following example, the areas where eligible_area is 0 are less prone to prediction errors than areas with code 1. Future: Later, we can add a check to see if the variable is a categorical or numerical variable.","title":"Structural Data Analysis"},{"location":"steps/error/#most-important-structural-variables","text":"Individual exploration might be interesting but can also be very time consuming. There is an option to get an overview of which features are most linearly related to the prediction errors. The features are then listed in the order of importance. There is a provision to choose how many variables to show. Explain how they have been calculated.","title":"Most important Structural Variables"},{"location":"steps/home/","text":"Home Page: Unemployment Rate Ranking The home page allows you to get quick takeaways based on the latest prediction results. The main feature of this page is to quickly see which kreis (or which group of kreise) are expected to have the highest unemployment rate for the next quarter. Kreise Ranking The first section is kreise ranking, containing three elements: Slide bar The slide bar allows you to see the top N kreise with the highest unemployment rates. It is set in a range from 10 to 100, in increments of 10. The default is set at 10. The screenshot sample is set at 30, meaning that it would show a dataframe with the top 30 highest unemployment rates. Sort-by Columns This is a multi-selection input field where you can choose multiple columns you want to sort the kreise ranking by. The ranking would then be sorted based on your input selections. The default is set to sort by three columns, in the order of: this quarter ( yyyy-mm-dd ), last quarter ( last_time% ), and the same quarter last year ( last_year% ). It is important to note that the order of your input selection matters: The input order means that it would first sort by the first input column, and then sort by the second input column if two kreis has the same result for the first input column. The input order also means that the top N kreis filtering is based on the first input column. Using the default selections as an example, the filtered result is the top N kreis based on unemployment rates of this quarter. However, the top N kreis last quarter may be a different 10 kreise. In order to get those top N kreise, you would have to put \"last quarter\" as the first sorting column in the input field. ^ Does this make sense? The available sort by options are: metedata index: state ( bundesland ) and county ( kreis ) all the input dates ( yyyy-mm-dd ) derived calculations: last quarter ( last_time% ): (this_Q - last_Q) / (this_Q) * 100 last year ( last_year% ): (this_Q - this_Q_last_year) / (this_Q) * 100 For example, the screenshot sample is set at the three default columns: this quarter ( 2020-03-31 ), last quarter ( last_time% ), and the same quarter last year ( last_year% ). Results After setting the number of top kreise you want to see and the columns you want to sort by, you are presented with the dataframe output of your choosing. This dataframe currently shows the top 30 kreis based on their unemployment rate this quarter, and also showing percentage change comparision to last quarter and last year, because they were in the input fields. Pro Tip : Click on a column to sort by the column. Click again to switch between ascending and descending sorting. The arrow circled in red next to the column means that the dataframe is currently sorted by that column. The arrow facing downwards means that the column is sorted in descending order. Note that even when the dataframe is sorted by the column you click, the top results is still filtered from the full dataframe based on the first input column. ^ Does this make sense? This dataframe currently shows the top 30 kreis based on their unemployment rate this quarter, and is also sorted by the percentage change compared to last year. For example, even though the dataframe is currently sorted by last_year% when we clicked on it, it is not sorting the top 30 kreis of last_year% based on the full dataset, but sorting based on the top 30 kreis of this quarter. Should I add a map highlighting the selected top kreis with annotations, and also colored by the first sorted column. Bundesland / Group Ranking The second section is bundesland / group ranking, containing five elements, following a similar format to the previous section: Slide-bar The slide bar allows you to see the top N kreise with the highest unemployment rates. It is set in a range from 50 to 200, in increments of 10. The default is set at 50. Compared to the previous section, the slide bar range is slightly larger because the reuslts in this section is grouped. Sort-by Columns Similar to the last section, the ranking would then be sorted based on your input column. Different from the last section, this section only allows you to sort by one column for simplicity. Group-by columns This multi-selection box offers a range of categorical variables to group by. The options include: growth_shrink_cities (\"wachsende/schrumpfende Kreise\") east_west (\"West-/Ostdeutschland\") labor_market_type (\"IAB-Arbeitsmarkttyp der Arbeitsagentur\") settlement_type_of_labor_market_region (\"Siedlungsstrukturtyp der Arbeitsmarktregion\") district_settlement_structure (\"Siedlungsstruktureller Kreistyp\") type_of_settlement_structure (\"Siedlungsstruktureller Regionstyp\") urban_rural (\"St\u00e4dtischer Raum / L\u00e4ndlicher Raum\") metropolitan_region (\"Europ\u00e4ische Metropolregion\") state area code (Amtlicher Gemeindeschl\u00fcssel, \" ags2 \") / state-level (\" bundesland \") ^ Are there other groupings that could be useful that are currently not included? Need to add GRW eligible for funding category column. The default is set to group by both growth_shrink_cities and east_west . Results The filtering result is a multi-index dataframe with three columns. The multi-index is presented based on how many columns you group by. The three columns include: {sort_by_column} : the column to sort by in the filtering dataframe. The default is set to sort by its percentage change compared to last_year% . #kreis : the number of kreise in that grouping. %count : the proportion of kreise belonging to that sorted grouping. ^ Change column name \"kreis\" to \"#kreis\" For example, this sample dataframe shows the top 50 kreise, sorted by last_year% , grouped by growth_shrink_cities and east_west . Reading the first row: the first column, `last_year%`, means that in the top 50 kreise with highest percentage change in unemployment rate compared to last year, `22` of them belong to growing cities in west Germany. the second column, `#kreis` means that there is a total of `163` (out of all 401) kreise that are growing cities in west Germany. the third column, `%counts`, means that 22 kreise accounts for `13.5%` of all the kreise in the growing-cities-West-Germany group. The #kreis and %counts are reference indicators to help contextualize the counts in the first column. For example, looking at the first column alone, it seems that growing-cities-West-Germany has the most number of kreise in the top 50 to have high unemployment changes since last year. However, when looking at the third column, we see that growing-above-average-West-Germany is actually the group with the highest percentage of kreise than other group to have the high unemployment changes in a year. Also note the number of multi-indices shown in the example. Since growth_shrink_cities have five categories, and east_west has two categories, there should be a total of 10 category groups in the index rows. However, the reason why not all combinations are shown is because some categories do not have kreise in it. Sometimes, it could be useful to see what category groups are not in the top lists. Visualizations Visualize the dataframe output results. As explained in the tip, when grouping by multiple columns, resulting a large number of combinations, it may be hard to see the results clearly using the pie chart or bar chart. Pie Chart The pie chart visualizes the {sort_by_column} into proportions. As shown above, the sample pie chart visualizes the percentage each category group takes in total from the last_year% column. For example, growing-cities-West-Germany group accounts for 22 out of the total of 50 top kreise, therefore, it takes up 44% as shown in the pie chart. Bar Chart As explained earlier, the pie chart could be a biased understanding of the category groups, and that can be balanced by understanding the percentage of those kreis accounting for the whole category group. The bar chart visualizes the %counts column, and draws a horizontal line on the 50% mark. As shown above, the sample bar chart visualizes the percentage the top 50 kreise took up for its whole category group. Note that you could use the two arrows on the top right to expand the plot if the display is too small on your screen. ^ Also add this as a pro tip on the tool page ^ Are there other things that should be added on the home page for quick access?","title":"Home page"},{"location":"steps/home/#home-page-unemployment-rate-ranking","text":"The home page allows you to get quick takeaways based on the latest prediction results. The main feature of this page is to quickly see which kreis (or which group of kreise) are expected to have the highest unemployment rate for the next quarter.","title":"Home Page: Unemployment Rate Ranking"},{"location":"steps/home/#kreise-ranking","text":"The first section is kreise ranking, containing three elements:","title":"Kreise Ranking"},{"location":"steps/home/#slide-bar","text":"The slide bar allows you to see the top N kreise with the highest unemployment rates. It is set in a range from 10 to 100, in increments of 10. The default is set at 10. The screenshot sample is set at 30, meaning that it would show a dataframe with the top 30 highest unemployment rates.","title":"Slide bar"},{"location":"steps/home/#sort-by-columns","text":"This is a multi-selection input field where you can choose multiple columns you want to sort the kreise ranking by. The ranking would then be sorted based on your input selections. The default is set to sort by three columns, in the order of: this quarter ( yyyy-mm-dd ), last quarter ( last_time% ), and the same quarter last year ( last_year% ). It is important to note that the order of your input selection matters: The input order means that it would first sort by the first input column, and then sort by the second input column if two kreis has the same result for the first input column. The input order also means that the top N kreis filtering is based on the first input column. Using the default selections as an example, the filtered result is the top N kreis based on unemployment rates of this quarter. However, the top N kreis last quarter may be a different 10 kreise. In order to get those top N kreise, you would have to put \"last quarter\" as the first sorting column in the input field. ^ Does this make sense? The available sort by options are: metedata index: state ( bundesland ) and county ( kreis ) all the input dates ( yyyy-mm-dd ) derived calculations: last quarter ( last_time% ): (this_Q - last_Q) / (this_Q) * 100 last year ( last_year% ): (this_Q - this_Q_last_year) / (this_Q) * 100 For example, the screenshot sample is set at the three default columns: this quarter ( 2020-03-31 ), last quarter ( last_time% ), and the same quarter last year ( last_year% ).","title":"Sort-by Columns"},{"location":"steps/home/#results","text":"After setting the number of top kreise you want to see and the columns you want to sort by, you are presented with the dataframe output of your choosing. This dataframe currently shows the top 30 kreis based on their unemployment rate this quarter, and also showing percentage change comparision to last quarter and last year, because they were in the input fields. Pro Tip : Click on a column to sort by the column. Click again to switch between ascending and descending sorting. The arrow circled in red next to the column means that the dataframe is currently sorted by that column. The arrow facing downwards means that the column is sorted in descending order. Note that even when the dataframe is sorted by the column you click, the top results is still filtered from the full dataframe based on the first input column. ^ Does this make sense? This dataframe currently shows the top 30 kreis based on their unemployment rate this quarter, and is also sorted by the percentage change compared to last year. For example, even though the dataframe is currently sorted by last_year% when we clicked on it, it is not sorting the top 30 kreis of last_year% based on the full dataset, but sorting based on the top 30 kreis of this quarter. Should I add a map highlighting the selected top kreis with annotations, and also colored by the first sorted column.","title":"Results"},{"location":"steps/home/#bundesland-group-ranking","text":"The second section is bundesland / group ranking, containing five elements, following a similar format to the previous section:","title":"Bundesland / Group Ranking"},{"location":"steps/home/#slide-bar_1","text":"The slide bar allows you to see the top N kreise with the highest unemployment rates. It is set in a range from 50 to 200, in increments of 10. The default is set at 50. Compared to the previous section, the slide bar range is slightly larger because the reuslts in this section is grouped.","title":"Slide-bar"},{"location":"steps/home/#sort-by-columns_1","text":"Similar to the last section, the ranking would then be sorted based on your input column. Different from the last section, this section only allows you to sort by one column for simplicity.","title":"Sort-by Columns"},{"location":"steps/home/#group-by-columns","text":"This multi-selection box offers a range of categorical variables to group by. The options include: growth_shrink_cities (\"wachsende/schrumpfende Kreise\") east_west (\"West-/Ostdeutschland\") labor_market_type (\"IAB-Arbeitsmarkttyp der Arbeitsagentur\") settlement_type_of_labor_market_region (\"Siedlungsstrukturtyp der Arbeitsmarktregion\") district_settlement_structure (\"Siedlungsstruktureller Kreistyp\") type_of_settlement_structure (\"Siedlungsstruktureller Regionstyp\") urban_rural (\"St\u00e4dtischer Raum / L\u00e4ndlicher Raum\") metropolitan_region (\"Europ\u00e4ische Metropolregion\") state area code (Amtlicher Gemeindeschl\u00fcssel, \" ags2 \") / state-level (\" bundesland \") ^ Are there other groupings that could be useful that are currently not included? Need to add GRW eligible for funding category column. The default is set to group by both growth_shrink_cities and east_west .","title":"Group-by columns"},{"location":"steps/home/#results_1","text":"The filtering result is a multi-index dataframe with three columns. The multi-index is presented based on how many columns you group by. The three columns include: {sort_by_column} : the column to sort by in the filtering dataframe. The default is set to sort by its percentage change compared to last_year% . #kreis : the number of kreise in that grouping. %count : the proportion of kreise belonging to that sorted grouping. ^ Change column name \"kreis\" to \"#kreis\" For example, this sample dataframe shows the top 50 kreise, sorted by last_year% , grouped by growth_shrink_cities and east_west . Reading the first row: the first column, `last_year%`, means that in the top 50 kreise with highest percentage change in unemployment rate compared to last year, `22` of them belong to growing cities in west Germany. the second column, `#kreis` means that there is a total of `163` (out of all 401) kreise that are growing cities in west Germany. the third column, `%counts`, means that 22 kreise accounts for `13.5%` of all the kreise in the growing-cities-West-Germany group. The #kreis and %counts are reference indicators to help contextualize the counts in the first column. For example, looking at the first column alone, it seems that growing-cities-West-Germany has the most number of kreise in the top 50 to have high unemployment changes since last year. However, when looking at the third column, we see that growing-above-average-West-Germany is actually the group with the highest percentage of kreise than other group to have the high unemployment changes in a year. Also note the number of multi-indices shown in the example. Since growth_shrink_cities have five categories, and east_west has two categories, there should be a total of 10 category groups in the index rows. However, the reason why not all combinations are shown is because some categories do not have kreise in it. Sometimes, it could be useful to see what category groups are not in the top lists.","title":"Results"},{"location":"steps/home/#visualizations","text":"Visualize the dataframe output results. As explained in the tip, when grouping by multiple columns, resulting a large number of combinations, it may be hard to see the results clearly using the pie chart or bar chart.","title":"Visualizations"},{"location":"steps/home/#pie-chart","text":"The pie chart visualizes the {sort_by_column} into proportions. As shown above, the sample pie chart visualizes the percentage each category group takes in total from the last_year% column. For example, growing-cities-West-Germany group accounts for 22 out of the total of 50 top kreise, therefore, it takes up 44% as shown in the pie chart.","title":"Pie Chart"},{"location":"steps/home/#bar-chart","text":"As explained earlier, the pie chart could be a biased understanding of the category groups, and that can be balanced by understanding the percentage of those kreis accounting for the whole category group. The bar chart visualizes the %counts column, and draws a horizontal line on the 50% mark. As shown above, the sample bar chart visualizes the percentage the top 50 kreise took up for its whole category group. Note that you could use the two arrows on the top right to expand the plot if the display is too small on your screen. ^ Also add this as a pro tip on the tool page ^ Are there other things that should be added on the home page for quick access?","title":"Bar Chart"},{"location":"steps/model/","text":"","title":"Model page"}]}